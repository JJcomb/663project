{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def particle_filtering(inpt,kbar,data,A_template,B):\n",
    "    ## Initialization\n",
    "    #B = Number of Samples drawn for each T\n",
    "    sigma = inpt[3]/np.sqrt(252)\n",
    "    k2 = 2**kbar\n",
    "    A = transition_mat(A_template.copy(),inpt,kbar)\n",
    "    g_m = gofm(inpt,kbar)\n",
    "    T = len(data)\n",
    "    # For storing pi_t\n",
    "    pi_mat = np.zeros((T+1,k2))\n",
    "    # For storing M_t\n",
    "    M_mat = np.zeros((T+1,B))\n",
    "    sim_like = np.zeros(T)\n",
    "    pi_mat[0,:] = (1/k2)*np.ones((1,k2))\n",
    "    \n",
    "    \"\"\"\n",
    "    Likelihood Algorithm\n",
    "    \"\"\"\n",
    "    pa = (2*np.pi)**(-0.5)\n",
    "    s = sigma*g_m\n",
    "    w_t = data \n",
    "    #w_t has to be 1x8 matrix\n",
    "    w_t = pa*np.exp(-0.5*((w_t/s)**2))/s\n",
    "    w_t = w_t + 1e-16\n",
    "    \n",
    "    # Reference dictionary to match the value of g_m and its corresponding likelihood for resampling\n",
    "    dict_ref = {val:w_t[key] for key, val in enumerate(g_m)}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    M- Sampling\n",
    "    \"\"\"\n",
    "    M_mat[0,:] = np.random.choice(g_m, size=B, replace=True, p=pi_mat[0,:])\n",
    "    for i in range(T):\n",
    "        # 1x8 matrix\n",
    "        temp_pi = pi_mat[i,:] @ A\n",
    "        \n",
    "        # element multiplication\n",
    "        ## Dimension of w_t and temp_pi has to be (1,k_bar)\n",
    "        pi_mat[i+1,:] = w_t * temp_pi\n",
    "        \n",
    "        # temporary sampling using the updated conditional distribution  (1000 samples)\n",
    "        temp_M = np.random.choice(g_m, size=B, replace=True, p=pi_mat[i+1,:])\n",
    "        # storing likelihoods that correspond to one of the values in \"g_m\"\n",
    "        temp_like = np.array([dict_ref[val] for val in temp_M])\n",
    "        # re_calculate the weighted probability.\n",
    "        w_like = temp_like/np.sum(temp_like)\n",
    "        M_mat[i+1,:]= np.random.choice(temp_M,size=B,replace=True,p=w_like)\n",
    "        \n",
    "        '''\n",
    "        Simulated Likelihood\n",
    "        '''\n",
    "        # Corresponding likelihood for each value in \"M_mat\" at time t\n",
    "        cond_dens = np.array([dict_ref[val] for val in M_mat[i+1,:]])\n",
    "        sim_like[i] = np.sum(cond_dens)/B\n",
    "    log_likelihood = np.sum(np.log(sim_like))\n",
    "    return (pi_mat, M_mat, log_likelihood)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Justification of Particle Filtering Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following proof provides the justification of the particle filtering we used in the code for approximating $\\Pi_{t+1}$ with two stage $M_{T+1}$ sampling.  \n",
    "Let F a function that maps $\\mathbb{R}$ to $\\mathbb{R}_{+}^{\\bar{k}}$.  \n",
    "Let d be the number of possible values that $m^{j}$ can take and $R_{t+1} \\equiv \\{r_s\\}_{s=1}^{t+1}$.  \n",
    "Let $h(m) \\equiv P(M_{t+1}|R_t)$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[F(M_{t+1})|R_{t+1}] &= \\sum_{j=1}^d P(M_{t+1}=m^j|R_{t+1})F(m^j) \\\\\n",
    "&= \\sum_{i=1}^d h(m^j)\\frac{P(M_{t+1}=m^j|R_{t+1})}{h(m^j)}F(m^j) \\\\\n",
    "&\\because h(m^j) = P(M_{t+1} = M^j|R_t), \\\\\n",
    "&\\text{by the convergence of the Monte Carlo Integration}, \\\\\n",
    "E[F(M_{t+1})|R_{t+1}] &\\approx \\frac{1}{B}\\sum_{b=1}^B \\frac{P(M_{t+1}=\\hat{M}_{t+1}^{(b)}|R_{t+1})}{h(\\hat{M}_{t+1}^{(b)})} F(\\hat{M}_{t+1}^{(b)}) \\\\\n",
    "&\\text{by Bayes Rule, the approximation above turns out to be importance sampling} \\\\\n",
    "&\\because \\frac{P(M_{t+1}=\\hat{M}_{t+1}^{(b)}|R_{t+1})}{Bh(\\hat{M}_{t+1}^{(b)})} = \\frac{f_{r_{t+1}}(r_{t+1}|M_{t+1}=\\hat{M}_{t+1}^{(b)})}{Bf_{r_{t+1}}(r_{t+1}|R_t)} \\\\\n",
    "&\\text{Moreover, the denominator can be approximated as follows} \\\\\n",
    "f_{r_{t+1}}(r_{t+1}|R_t) &\\approx \\frac{1}{B}\\sum_{b=1}^B f_{r_{t+1}}(r_{t+1}|\\hat{M}_{t+1}^{(b)}) \\\\\n",
    "&\\therefore E[F(M_{t+1})|R_{t+1}] \\approx \\sum_{b=1}^B \\frac{f_{r_{t+1}}(r_{t+1}|M_{t+1}=\\hat{M}_{t+1}^{(b)})}\n",
    "{\\sum_{b=1}^B f_{r_{t+1}}(r_{t+1}|\\hat{M}_{t+1}^{(b)})} F(\\hat{M}_{t+1}^{(b)}) \\\\\n",
    "&= \\sum_{b=1}^B W_b F(\\hat{M}_{t+1}^{(b)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "As shown above, The expectation $E[F(M_{t+1})|R_{t+1}]$ can be approximated with the weighted average of the statistics of drawn samples, weighted by the likelihoods of the returns. This resembles the form of approximation using importance sampling. Since, $F$ function was defined as a mapping from the real number space to the positive real value space, $F$ can be replaced with the state conditional probability $\\Pi_{t+1}$. Therefore, the proof demonstrates that the code implementation of our particle filtering can be justfied to approximate $\\Pi_{t+1}$ with re-sampled $\\{\\hat{M}_{t+1}^{(i)}  \\}_{i=1}^B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure of Particle Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At fixed time $t$,  \n",
    "i) Draw $\\{M_{t+1}^{(i)}\\}_{i=1}^B$ using $P(M_{t+1}^{i}|M_{t}^{i})$(Transition Matrix).  \n",
    "ii) Reweight the probability of draws using importance sampling and re-draw $\\{\\hat{M}_{t+1}^{(i)}\\}_{i=1}^B$ using those reweighted probabilities.\n",
    "  \n",
    "\n",
    "Assume $\\{M_{t}^{(i)}\\}_{i=1}^B$ have been independently drawn from $\\Pi_t$\n",
    "\n",
    "$\\Pi_{t+1}$ can be updated given a new return $r_{t+1}$ as follows.  \n",
    "$\\Pi_{t+1}^j \\propto f_{r_{t+1}}(r_{t+1}|M_{t+1}=m^j)\\sum_{i=1}^d P(M_{t+1}=m^j|M_t=m^i)\\Pi_t^i$  \n",
    "\n",
    "In the first step, draw $M_{t+1}^{(i)}$ given $M_{t}^{(i)}$ using one-step ahead Marcov chain property of $M_t$ Repeat this B times, then first stage of $\\{M_{t+1}^{(i)}\\}_{i=1}^B$ are obtained. However, the samples drawn given $M_t$ uses information available up to time t only. Therefore, it is necessary to update $M_{t+1}$ incorporating the information of the return at $t+1$ in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fixed sample $b$, calculate $P(q=b) = \\frac{f_{r_{t+1}}(r_{t+1}|M_{t+1}=M_{t+1}^{(b)})}{\\sum_{a=1}^B f_{r_{t+1}}(r_{t+1}|M_{t+1} = M_{t+1}^{(a)})}$.  \n",
    "Then draw B samples of $M_{t+1}$ from $\\{M_{t}^{(i)}\\}_{i=1}^B$ with corresponding weighted probabilities and let them be defined as $\\{\\hat{M}_{t+1}^{(i)}\\}_{i=1}^{B}$. Then it will follow conditional distribution $\\Pi_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Particle Filtering Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $\\{M_{T}^{(i)}\\}_{i=1}^B$ is sampled correctly using the method above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fixed time $T$,  \n",
    "i) Draw $\\{M_{T+1}^{(b)}\\}_{b=1}^B$ using $P(M_{T+1}^{(b)}|M_{T}= M_{T}^{(b)})$.   \n",
    "ii) Calculate the probability of draws using importance sampling and obtain $\\tilde{M}_{T+1}$ using those probabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, draw $M_{T+1}^{(b)}$ using one-step forward Marcov chain property of $M_t$. After repeating this sampling for $\\forall b$, we are able to obtain temporary samples of $\\{\\hat{M}_{T+1}^{(i)}\\}_{i=1}^B$. \n",
    "In the second stage of sampling, we calculate the weight $P(q=b) = \\frac{f_{r_{T}}(r_{T}|M_{t+1}=M_{T+1}^{(b)})}{\\sum_{a=1}^B f_{r_{T}}(r_{T}|M_{T+1} = M_{T+1}^{(a)})}$ for $\\forall b$. Unlike particle filtering for sampling $M_t$ up to t=T where the likelihoods of one period forward returns are used to calculate the weights of each sample,  we are using the likelihood of the return at current time $t=T$ because we do not have $r_{T+1}$ at $t=T$. Finally, the weighted average $\\tilde{M}_{T+1} = \\sum_{b=1}^B p(q=b)\\hat{M}_{T+1}^{(b)}$ can be obtained with samples' corresponding probabilities.  \n",
    "Since the likelihood of the return at $t=T+1$ is determined by $M_{T+1}$, predicting the stochastic volatility index $M_{T+1}$ is equivalent to implementing a prediction on the return $r_{T+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Performance of Garch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reschenhofer, Erhard. \"Does Anyone Need a GARCH(1,1)?.\" Journal of Finance and Accounting 1.2 (2013): 48-53.  \n",
    "  \n",
    "  \n",
    "Hansen, P. R., & Lunde, A. (2005). A forecast comparison of volatility models: does anything beat a GARCH (1, 1)?. Journal of applied econometrics, 20(7), 873-889.  \n",
    "  \n",
    "  \n",
    "Zivot, Eric \"Practical Issues in the Analysis of Univariate GARCH Models\"(2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garch model has been widely used for volatility forecasting for the last few decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is \"Long-Memory\" property that both MSM and GARCH have in common, therefore is the reason why we chose the model to compare the performance with MSM model.  \n",
    "In MSM model, the author defines transition probabilities $\\gamma \\equiv (\\gamma_1,\\gamma_2,...,\\gamma_{\\bar{k}})$ where $\\gamma_k = 1-(1-\\gamma_1)^{(b^{k-1})} \\approx \\gamma_1 b^{k-1}$. Here, $b$ determines the degree of long-memory property(autocorrelation across time).  \n",
    "Similarly, GARCH also has long memory behavior. Zivot \"Practical Issues in the Analysis of Univariate GARCH Models\" demonstrates empirical evidence of long memory behavior with S&P 500 and Microsoft stock returns. According to the paper, $\\lim_{k\\to\\infty}\\rho(k)  = C_{\\rho}k^{2d-1}$, where $C_{\\rho}$ is a positive constant, $d\\in [0,\\frac{1}{2}]$ and $\\rho(k)$ is an autocorrelation function clarifies that the autocorrelation function that has a long memory process decays slowly at a hyperbolic rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There has been debates over the model selection which can be narrowed down to the selection of \"p\" and \"q\".  \n",
    "Hansen & Lunde \"Does anything beat a GARCH(1,1)?\" state that no other model performs better in forecasting than the GARCH (1,1) model after comparing a large number of parametric volatility models in the study. On the other hand, Reschenhofer claims simple estimators such as weighted medians of past returns perform better than the GARCH(1,1) does. Therefore,  $(p,q) =(2,3)$ which was selected after comparing AIC among several parametric models and GARCH(1,1) are used to compare the performances with that of MSM model in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
